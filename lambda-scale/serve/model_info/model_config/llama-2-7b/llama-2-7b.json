{
    "model_name": "llama-2-7b",
    "original_block" : [0],
    "transfer_block_num": 16,
    "block_num": 8,
    "gpu_num": 1,
    "device_distribution": [
      [0,1,2,3,4,5,6,7]
    ],
    "block_storage_size" : [1881210880,
    1619066880,
    1619066880,
    1619066880,
    1619066880,
    1619066880,
    1619066880,
    1881219072],
    "decode_time" : 0.014,
    "block_execute_distribution" : [8,8,8,8,8,8,8,8],
    "blocks": [
      {
        "block_id": 0,
        "layer_list": [0,1,2,3]
      },
      {
        "block_id": 1,
        "layer_list": [4,5,6,7]
      },
      {
        "block_id": 2,
        "layer_list": [8,9,10,11]
      },
      {
        "block_id": 3,
        "layer_list": [12,13,14,15]
      },
      {
        "block_id": 4,
        "layer_list": [16,17,18,19]
      },
      {
        "block_id": 5,
        "layer_list": [20,21,22,23]
      },
      {
        "block_id": 6,
        "layer_list": [24,25,26,27]
      },
      {
        "block_id": 7,
        "layer_list": [28,29,30,31]
      }
    ],
    "forward_dag": {
      "0": [1],
      "1": [2],
      "2": [3],
      "3": [4],
      "4": [5],
      "5": [6],
      "6": [7],
      "7": []
    },
    "backward_dag": {
      "7": [6],
      "6": [5],
      "5": [4],
      "4": [3],
      "3": [2],
      "2": [1],
      "1": [0],
      "0": []
    }
  }