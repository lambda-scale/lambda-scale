{
    "model_name": "llama-2-70b",
    "original_block" : [0],
    "transfer_block_num": 16,
    "block_num": 8,
    "gpu_num": 4,
    "device_distribution": [
      [0,1],
      [2,3],
      [4,5],
      [6,7]
    ],
    "decode_time" : 0.14,
    "block_storage_size" : [17637376000,
    17113088000,
    17113088000,
    17113088000,
    17113088000,
    17113088000,
    17113088000,
    17637392384],
    "block_execute_distribution" : [8,8,8,8,8,8,8,8],
    "blocks": [
      {
        "block_id": 0,
        "layer_list": [0,1,2,3,4,5,6,7,8,9]
      },
      {
        "block_id": 1,
        "layer_list": [10,11,12,13,14,15,16,17,18,19]
      },
      {
        "block_id": 2,
        "layer_list": [20,21,22,23,24,25,26,27,28,29]
      },
      {
        "block_id": 3,
        "layer_list": [30,31,32,33,34,35,36,37,38,39]
      },
      {
        "block_id": 4,
        "layer_list": [40,41,42,43,44,45,46,47,48,49]
      },
      {
        "block_id": 5,
        "layer_list": [50,51,52,53,54,55,56,57,58,59]
      },
      {
        "block_id": 6,
        "layer_list": [60,61,62,63,64,65,66,67,68,69]
      },
      {
        "block_id": 7,
        "layer_list": [70,71,72,73,74,75,76,77,78,79]
      }
    ],
    "forward_dag": {
      "0": [1],
      "1": [2],
      "2": [3],
      "3": [4],
      "4": [5],
      "5": [6],
      "6": [7],
      "7": []
    },
    "backward_dag": {
      "7": [6],
      "6": [5],
      "5": [4],
      "4": [3],
      "3": [2],
      "2": [1],
      "1": [0],
      "0": []
    }
  }