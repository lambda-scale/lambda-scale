[APPLICATION]
#config.py file
#cuda device id
device_id = 0
workers_addr = 10.153.52.123:6000,10.153.52.186:6000,10.153.53.4:6000,10.153.53.5:6000
# worker log file name
worker_log_file = bcast_worker
# controller log file name
ctrl_log_file = bcast_ctrl

[Automation]
# change number of runs each time total_workers will multiple by 2
num_experiments = 1
# the starting point
total_workers = 2
ssh_port = 2222
local_rdmc_dir = /scratch/erc8gx/containers/workspace/RDMC-GDR
node_config_file = /scratch/erc8gx/containers/workspace/RDMC-GDR/src/conf/derecho_node-sample.cfg
node_dir = /scratch/qgh4hm/chaobo/slurm_nodes/slurm_node0
worker_comm_establish_wait_time = 5
worker_inter_wait_time = 4
controller_wait_time = 4
client_wait_time = 8


#[fgs | gfs]
target_app = gfs
#[uva | ali]
resource_platform = uva

[P2P]
# my local id - each node should have a different id
my_id = 0
# my local ip address
my_ip = 10.153.52.123 
# the leader node to send my local information and wait others to join the group
contact_ip = 
# view port
view_port = 23580
# p2p tcp port
p2p_port = 31675
# total p2p instances in cluster
total_p2p_nodes = 4
# logging level
default_log_level = debug
# prefix for naming differnt application log files
log_file_prefix = fgs



[DERECHO]
# my local id - each node should have a different id
local_id = 0
# my local ip address
local_ip = 127.0.0.1
# These ports are optional: nodes will use the values from the group derecho.cfg by default,
# but if the port options are specified here they will override the defaults.
# derecho gms port
gms_port = 23580
# derecho state-transfer port
state_transfer_port = 28366
# sst tcp port
sst_port = 37683
# rdmc tcp port
rdmc_port = 31675
# externel tcp port listening to external clients
external_port = 32645


# RDMA section contains configurations of the following
# - which RDMA device to use
# - device configurations
[RDMA]
# 1. provider = bgq|gni|efa|hook|netdir|psm|psm2|psm3|rxd|rxm|shm|udp|usnic|verbs
# possible options(only 'sockets' and 'verbs' providers are tested so far):
# bgq     - The Blue Gene/Q Fabric Provider
# efa     - The Amazon Elastic Fabric Adapter
# gni     - The GNI Fabric Provider (Cray XC (TM) systems)
# hook    - The Hook Fabric Provider Utility
# netdir  - The Network Direct Fabric Provider (Microsoft Network Direct SPI)
# psm     - The PSM Fabric Provider
# psm2    - The PSM2 Fabric Provider
# psm3    - The PSM3 Fabric Provider
# rxd     - The RxD (RDM over DGRAM) Utility Provider
# rxm     - The RxM (RDM over MSG) Utility Provider
# shm     - The SHM Fabric Provider
# tcp     - The TCP Fabric Provider
# udp     - The UDP Fabric Provider
# usnic   - The usNIC Fabric Provider (Cisco VIC)
# verbs   - The Verbs Fabric Provider
# Please note that only "tcp" and "verbs" are tested this moment.
provider = verbs

# 2. domain
# For sockets provider, domain is the NIC name (ifconfig | grep -v -e "^ ")
# For verbs provider, domain is the device name (ibv_devices)
domain = mlx5_2

# 3. tx_depth
# tx_depth applies to hints->tx_attr->size, where hint is a struct fi_info object.
# see https://ofiwg.github.io/libfabric/master/man/fi_getinfo.3.html
tx_depth = 4096

# 4. rx_depth:
# rx_depth applies to hints->rx_attr->size, where hint is a struct fi_info object.
# see https://ofiwg.github.io/libfabric/master/man/fi_getinfo.3.html
rx_depth = 4096
