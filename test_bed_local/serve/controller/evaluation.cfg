# system config
total_node_num = 8
total_gpu_num = 1
root_path = /scratch/infattllm/fgscaling/rui/slurm_nodes/slurm_node0/src
is_rdma = True
host_memory_size = 30000000000

#baseline switch
is_nccl_impl = False

is_nccl = False
is_faasnet = False
is_sllm = False

is_ideal = False

is_sys_ablation_tensor_pack = False
is_sys_ablation_pre_alloc = False
is_sys_ablation_memory_gdr = False

# evaluation config

# base setup
model_name = llama-2-13b
model_id = 0

if_init_data = True
init_node_num = 1

default_ssd = True
memory_num = 0

default_memory = False
default_remote_storage = False

is_memory_keep_alive = False
memory_keep_alive_time = 0

# fixed scaling
fixed_evaluation = True
scale_node_num = 7

# trace
is_trace = False
# fixed request
rps = 0

# switch
is_disable_cache = False
is_disable_execute = True
is_disable_reorder = False
is_half_reorder = False
